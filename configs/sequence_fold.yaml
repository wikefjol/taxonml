# configs/sequence_fold_full.yaml

experiment:
  name: "sequence_fold_full"
  seed: 42
  profile_default: "full"   # "full" | "debug"

artifacts:
  roots:
    experiments: "/mimer/NOBACKUP/groups/snic2022-22-552/filbern/fungal_classification/experiments"
    pretrained:  "/mimer/NOBACKUP/groups/snic2022-22-552/filbern/fungal_classification/pretrained_models"
  paths:
    # ---- used by 01_experiment_prep.py ----
    prepared_dir:        "{experiments_root}/{experiment_name}/data"
    logs_dir:            "{experiments_root}/{experiment_name}/logs"
    prep_log:            "{experiments_root}/{experiment_name}/logs/prep.log"
    prep_summary_json:   "{experiments_root}/{experiment_name}/data/{prep_summary_filename}"

    # ---- shared helpers for pretrain/finetune (you can use these or compute in code) ----
    arch_root:             "{experiments_root}/{experiment_name}/{arch_id}/{profile}"
    pretrain_run_dir:      "{experiments_root}/{experiment_name}/{arch_id}/{profile}/pretrain"
    pretrain_ckpt_dir:     "{experiments_root}/{experiment_name}/{arch_id}/{profile}/pretrain/checkpoints"
    finetune_run_dir:      "{experiments_root}/{experiment_name}/{arch_id}/{profile}/folds/fold_{fold:02d}/{task}"
    finetune_ckpt_dir:     "{experiments_root}/{experiment_name}/{arch_id}/{profile}/folds/fold_{fold:02d}/{task}/checkpoints"

data:
  experiments_root: "${ARTIFACTS_EXPERIMENTS_ROOT:-/mimer/NOBACKUP/groups/snic2022-22-552/filbern/fungal_classification/experiments}"
  input_dir: "${UNION_INPUT_DIR:-/mimer/NOBACKUP/groups/snic2022-22-552/filbern/fungal_classification/inputs}"
  filenames:
    prep_summary: "prep_summary.json"
  # union membership for prep
  inputs:
    sequence_fold_full: ["blast_union_A", "blast_union_B"]  # csv basenames (without .csv)

label_space:
  levels: ["phylum","class","order","family","genus","species"]

prep:  # 01_experiment_prep.py only
  uppercase_sequences: true
  min_species_resolution: 0
  dedupe:
    scope: "within_species"
    keep_policy: "highest_resolution"   # or "first"/"last"
  folds:  # exp1: sequence-level stratified kfold
    k: 10
    stratify_by: "species"
    seed: 42
  folds_species_group:  # exp2: species-group, stratify by genus
    k: 10
    stratify_by: "genus"
    seed: 42
  debug_subset:
    enabled: false
    n_genera: 30
    min_species_per_genus: 5
    target_size: 20000
    seed: 42

preprocessing:  # shared by pretrain + finetune
  alphabet: ["A","C","G","T"]
  k: 3
  max_bases: 1500            # must be divisible by k; optimal_length=max_bases/k; MPE=optimal_length+2
  truncation: "sliding_window"  # informational; implementation is fixed today

model:  # shared encoder spec; arch_id derived from this + MPE
  encoder:
    hidden_size: 512
    num_hidden_layers: 10
    num_attention_heads: 8
    intermediate_size: 2048
    hidden_dropout_prob: 0.10
    attention_probs_dropout_prob: 0.10

scheduler_catalog:  # shared shapes; pick by key in each task section
  tri:
    base: "step"
    warmup: {type: "linear", duration: 1}
    main:   {type: "tri", plateau: 3, decay: 8}
    floor:  {min_factor: 1.0e-2}
  cosine:
    base: "step"
    warmup: {type: "linear", duration: 1}
    main:   {type: "cosine", epochs: 10}
    floor:  {min_factor: 1.0e-2}
  cosine_restarts:
    base: "step"
    warmup: {type: "linear", duration: 1}
    main:   {type: "cosine_restarts", cycle_epochs: 5, num_cycles: 3, t_mult: 1.0, peak_decay: 0.8}
    floor:  {min_factor: 1.0e-2}

tasks:

  pretrain:
    profile: "${PROFILE:-full}"    # runtime default
    fold_index_for_val: 7          # uses exp1 folds to carve train!=fold, val==fold
    mlm:
      masking_pct: 0.15
      mlm_dropout: 0.10
      tie_to_embeddings: true
    dataloader:
      batch_size: 126
      num_workers: 4
      pin_memory: true
      drop_last_train: true
      prefetch_factor: 4
    optimizer:
      name: "AdamW"
      learning_rate: 1.8e-4
      weight_decay: 0.01
      betas: [0.9, 0.999]
      eps: 1.0e-8
    scheduler:
      kind: "tri"  # key into scheduler_catalog
    trainer:
      max_epochs: 600
      amp: true
      log_every: 100
      save_every_epochs: 1
    promotion:   # where best.pt is copied
      pretrained_root: "{pretrained_root}/{arch_id}/best.pt"

  finetune:
    profile: "${PROFILE:-full}"
    levels: "all"                  # "all" or explicit list like ["species"] or ["genus","species"]
    fold_scheme: "exp1"            # "exp1" | "exp2"
    fold_index: 7
    head:
      hierarchical_dropout: 0.10
      bottleneck: 256
    backbone:
      path: "/mimer/NOBACKUP/groups/snic2022-22-552/filbern/fungal_classification/pretrained_models/bert_h512_L10_H8_i2048_P502/best.pt"  # null â†’ derive by arch_id
      strict: true
    dataloader:
      batch_size: 128
      num_workers: 4
      pin_memory: true
      sampler: "species_balanced"  # informational; current code constructs this internally
      drop_last_train: true
      prefetch_factor: 4
    optimizer:
      name: "AdamW"
      learning_rate: 1.0e-4
      weight_decay: 0.10
      betas: [0.9, 0.999]
      eps: 1.0e-8
    scheduler:
      kind: "tri"
    trainer:
      max_epochs: 10
      amp: true
      log_every: 5

profiles:  # optional profile-specific overrides merged on top of everything above
  debug:
    preprocessing:
      max_bases: 600
    model:
      encoder:
        hidden_size: 64
        num_hidden_layers: 2
        num_attention_heads: 2
        intermediate_size: 256
    tasks:
      pretrain:
        dataloader: {batch_size: 16, num_workers: 0, prefetch_factor: null, drop_last_train: true}
        optimizer: {learning_rate: 1.8e-4}
        trainer:   {max_epochs: 2, amp: false, log_every: 20}
      finetune:
        backbone:
          path: "/mimer/NOBACKUP/groups/snic2022-22-552/filbern/fungal_classification/pretrained_models/bert_h64_L2_H2_i256_P202/best_clean.pt"
          strict: true
        dataloader: {batch_size: 16, num_workers: 0, prefetch_factor: null, drop_last_train: true}
        trainer:   {max_epochs: 2, amp: false, log_every: 5}
